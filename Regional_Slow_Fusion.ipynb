{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slow Fusion\n",
    "\n",
    "This script takes all frames at once and passes them through a cnn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.models import Sequential, save_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, Conv3D, MaxPooling2D, MaxPooling3D, BatchNormalization\n",
    "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras import initializers\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Define Region\n",
    "\n",
    "First, define a region to train a model for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGIONS = [\"ES\",\"NS\",\"WS\",\"EM\",\"EE\",\"LD\",\"NEE\",\"NWE\",\"SEE\",\"SWE\",\"WAL\",\"WM\",\"YH\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data\n",
    "\n",
    "To begin, data for a middle frames model must be loaded along with the expected regional rainfall values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.0 New Training Data\n",
    "\n",
    "This section loads the new training data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_datafile = \"D:/PHD_DATA/Video_18-01-2021/prepared-data/middle_all.npy\"\n",
    "training_rainfallfile = \"D:/PHD_DATA/Video_18-01-2021/prepared-data/expected_all.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_videos = np.load(training_datafile)\n",
    "training_rainfall = np.load(training_rainfallfile)[:, 2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_videos = np.swapaxes(training_videos, 1, 2)\n",
    "training_videos = np.swapaxes(training_videos, 2, 3)\n",
    "training_videos = np.swapaxes(training_videos, 3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_videos[:, :, :, :, 0] = (training_videos[:, :, :, :, 0] - np.mean(training_videos[:, :, :, :, 0])) / (np.std(training_videos[:, :, :, :, 0]))\n",
    "training_videos[:, :, :, :, 1] = (training_videos[:, :, :, :, 1] - np.mean(training_videos[:, :, :, :, 1])) / (np.std(training_videos[:, :, :, :, 1]))\n",
    "\n",
    "for n in range(0, len(REGIONS)):\n",
    "    training_rainfall[:, n] = (training_rainfall[:, n] - np.min(training_rainfall[:, n])) / (np.max(training_rainfall[:, n]) - np.min(training_rainfall[:, n]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(np.min(training_rainfall, axis=0))\n",
    "print(np.max(training_rainfall, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_augmentation(raw_rain, raw_vids):\n",
    "    vids = np.copy(raw_vids)\n",
    "    vids = vids + np.random.normal(0, 0.05, vids.shape)\n",
    "    return np.concatenate((raw_vids, vids), axis=0), np.concatenate((raw_rain, raw_rain), axis=0)\n",
    "\n",
    "APPLY_AUGMENTATION = True\n",
    "\n",
    "if APPLY_AUGMENTATION:\n",
    "    training_videos, training_rainfall = apply_augmentation(training_rainfall, training_videos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(548, 28, 61, 121, 2)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_videos.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Definition\n",
    "\n",
    "Next, a CNN model architecture is defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SlowFusion(Model):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(SlowFusion, self).__init__()\n",
    "        \n",
    "        # First level\n",
    "        self._first_layers = [\n",
    "            self._generate_conv_layer(8, (2, 2, 2), (7, 61, 121, 2)),\n",
    "            self._generate_conv_layer(8, (2, 2, 2), (7, 61, 121, 2)),\n",
    "            self._generate_conv_layer(8, (2, 2, 2), (7, 61, 121, 2)),\n",
    "            self._generate_conv_layer(8, (2, 2, 2), (7, 61, 121, 2))\n",
    "        ]\n",
    "        \n",
    "        # Second level\n",
    "        self._second_layers = [\n",
    "            self._generate_conv_layer(8, (2, 2, 2), (6, 30, 60, 8)),\n",
    "            self._generate_conv_layer(8, (2, 2, 2), (6, 30, 60, 8))\n",
    "        ]\n",
    "        \n",
    "        # Third level\n",
    "        self._third_layers = [\n",
    "            self._generate_conv_layer(8, (2, 2, 2), (4, 14, 29, 8))\n",
    "        ]\n",
    "        \n",
    "\n",
    "        # Final Dense layer\n",
    "        self._final_layer = self._generate_dense_layer(1)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        outputs = self._layer_one(inputs)\n",
    "        #print(outputs[0].shape)\n",
    "        outputs = self._layer_two(outputs)\n",
    "        #print(outputs[0].shape)\n",
    "        outputs = self._layer_three(outputs)\n",
    "        #print(outputs.shape)\n",
    "        outputs = self._final_layer(outputs)\n",
    "        #print(outputs.shape)\n",
    "        return outputs\n",
    "        \n",
    "    def _layer_one(self, inputs):\n",
    "        outputs = []\n",
    "        for n in range(0, 4):\n",
    "            outputs.append(\n",
    "                self._first_layers[n](\n",
    "                    inputs[:, (n*7):((n+1)*7), :, :, :]\n",
    "                )\n",
    "            )\n",
    "        trans_outputs = [\n",
    "            tf.concat((outputs[0], outputs[1]), axis=1),\n",
    "            tf.concat((outputs[2], outputs[3]), axis=1)\n",
    "        ]\n",
    "        return trans_outputs\n",
    "    \n",
    "    def _layer_two(self, inputs):\n",
    "        outputs = []\n",
    "        for n in range(0, 2):\n",
    "            outputs.append(self._second_layers[n](inputs[n]))\n",
    "        trans_outputs = [\n",
    "            tf.concat((outputs[0], outputs[1]), axis=1)\n",
    "        ]\n",
    "        #trans_outputs = tf.concat((outputs[0], outputs[1]), axis=1)\n",
    "        return trans_outputs\n",
    "    \n",
    "    def _layer_three(self, inputs):\n",
    "        return self._third_layers[0](inputs)\n",
    "    \n",
    "    def _generate_conv_layer(self, filters, poolsize, input_shape):\n",
    "        layer = Sequential()\n",
    "        layer.add(Conv3D(\n",
    "            filters, poolsize, input_shape=input_shape)) # initializers.RandomNormal(mean=0.0, stddev=0.05, seed=None)))\n",
    "        #layer.add(BatchNormalization())\n",
    "        layer.add(Activation('relu'))\n",
    "        layer.add(MaxPooling3D(pool_size=poolsize))\n",
    "        return layer\n",
    "    \n",
    "    def _generate_dense_layer(self, output_size):\n",
    "        layer = Sequential()\n",
    "        layer.add(Flatten())\n",
    "        layer.add(Dense(output_size)) # initializers.RandomNormal(mean=0.0, stddev=0.05, seed=None)))\n",
    "        #layer.add(BatchNormalization())\n",
    "        layer.add(Activation('relu'))\n",
    "        return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_generator(input_shape=(28, 61, 121, 2), learning_rate=0.1):\n",
    "    \"\"\" This method generates a model definition. \"\"\"\n",
    "    model = SlowFusion()\n",
    "    \n",
    "    # Setup training mechanism\n",
    "    model.compile(\n",
    "        loss=\"mean_squared_error\",\n",
    "        optimizer=Adam(learning_rate=learning_rate))#SGD(lr=learning_rate, nesterov=True))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Training\n",
    "\n",
    "Finally, training the model using the single framed data and opening a tensorboard instance with details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(run_name, tensorboard, model, xdata, ydata, models_folder=\"D:/PHD_DATA/Video_18-01-2021/models/\"):\n",
    "    \"\"\" Trains the given model with the given dataset. \"\"\"\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n",
    "    history = model.fit(\n",
    "        xdata,\n",
    "        ydata,\n",
    "        batch_size=8,\n",
    "        validation_split=0.3,\n",
    "        callbacks=[tensorboard, es],\n",
    "        epochs=1\n",
    "    )\n",
    "    save_model(model, models_folder + run_name + \".mdl\")\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flexible parameters\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48/48 [==============================] - 41s 773ms/step - loss: 0.0457 - val_loss: 0.0402\n",
      "INFO:tensorflow:Assets written to: D:/PHD_DATA/Video_11-02-2021/SlowFusion/models/ES-1613064190.mdl\\assets\n",
      "21/48 [============>.................] - ETA: 17s - loss: 0.1978"
     ]
    }
   ],
   "source": [
    "final_errors = {}\n",
    "# Run model for each region\n",
    "for ridx, r in enumerate(REGIONS[:2]):\n",
    "    final_errors[r] = {}\n",
    "    # Run each model multiple times\n",
    "    for i in range(0, 1):\n",
    "        run_name = \"{}-{}\".format(r, int(time.time()))\n",
    "        tb = TensorBoard(log_dir=\"D:/PHD_DATA/Video_11-02-2021/SlowFusion/logs/{}\".format(run_name))\n",
    "        model = model_generator(learning_rate=learning_rate, input_shape=training_videos.shape[1:])\n",
    "        history = train_model(run_name, tb, model, training_videos, training_rainfall[:, ridx],\n",
    "                              models_folder=\"D:/PHD_DATA/Video_11-02-2021/SlowFusion/models/\")\n",
    "        final_errors[r][run_name] = history.history[\"val_loss\"][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
