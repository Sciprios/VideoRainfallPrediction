{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slow Fusion\n",
    "\n",
    "This script takes all frames at once and passes them through a cnn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.models import Sequential, save_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, Conv3D, MaxPooling2D, MaxPooling3D, LayerNormalization\n",
    "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import initializers\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data\n",
    "\n",
    "To begin, data for a middle frames model must be loaded along with the expected regional rainfall values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.0 New Training Data\n",
    "\n",
    "This section loads the new training data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_datafile = \"D:/PHD_DATA/Video_18-01-2021/prepared-data/middle_all.npy\"\n",
    "training_rainfallfile = \"D:/PHD_DATA/Video_18-01-2021/prepared-data/expected_all.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_videos = np.load(training_datafile)\n",
    "training_rainfall = np.load(training_rainfallfile)[:, 2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_videos = np.swapaxes(training_videos, 1, 2)\n",
    "training_videos = np.swapaxes(training_videos, 2, 3)\n",
    "training_videos = np.swapaxes(training_videos, 3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_videos[:, :, :, 0] = (training_videos[:, :, :, 0] - np.min(training_videos[:, :, :, 0])) / (np.max(training_videos[:, :, :, 0]) - np.min(training_videos[:, :, :, 0]))\n",
    "training_videos[:, :, :, 1] = (training_videos[:, :, :, 1] - np.min(training_videos[:, :, :, 1])) / (np.max(training_videos[:, :, :, 1]) - np.min(training_videos[:, :, :, 1]))\n",
    "\n",
    "#training_videos[:, :, :, 0] = (training_videos[:, :, :, 0] - np.mean(training_videos[:, :, :, 0])) / np.std(training_videos[:, :, :, 0])\n",
    "#training_videos[:, :, :, 1] = (training_videos[:, :, :, 1] - np.mean(training_videos[:, :, :, 1])) / np.std(training_videos[:, :, :, 1]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 DEPRECATED\n",
    "\n",
    "This following section uses two separate datasets. Which is not required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training_datafile = \"E:/31-12-2020/prepared-data/middle_train.npy\"\n",
    "#validation_datafile = \"E:/31-12-2020/prepared-data/middle_valid.npy\"\n",
    "#\n",
    "#training_rainfallfile = \"E:/31-12-2020/prepared-data/expected_train.npy\"\n",
    "#validation_rainfallfile = \"E:/31-12-2020/prepared-data/expected_valid.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training_videos = np.load(training_datafile)\n",
    "#validation_videos = np.load(validation_datafile)\n",
    "#\n",
    "#training_rainfall = np.load(training_rainfallfile)[:, 2:]\n",
    "#validation_rainfall = np.load(validation_rainfallfile)[:, 2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need them in X, Y, COLOURS\n",
    "#training_videos = np.swapaxes(training_videos, 1, 2)\n",
    "#training_videos = np.swapaxes(training_videos, 2, 3)\n",
    "#training_videos = np.swapaxes(training_videos, 3, 4)\n",
    "#\n",
    "#validation_videos = np.swapaxes(validation_videos, 1, 2)\n",
    "#validation_videos = np.swapaxes(validation_videos, 2, 3)\n",
    "#validation_videos = np.swapaxes(validation_videos, 3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data\n",
    "#training_videos[:, :, :, :, 0] = (training_videos[:, :, :, :, 0] - np.min(training_videos[:, :, :, :, 0])) / (np.max(training_videos[:, :, :, :, 0]) - np.min(training_videos[:, :, :, :, 0]))\n",
    "#training_videos[:, :, :, :, 1] = (training_videos[:, :, :, :, 1] - np.min(training_videos[:, :, :, :, 1])) / (np.max(training_videos[:, :, :, :, 1]) - np.min(training_videos[:, :, :, :, 1]))\n",
    "\n",
    "#validation_videos[:, :, :, :, 0] = (validation_videos[:, :, :, :, 0] - np.min(validation_videos[:, :, :, :, 0])) / (np.max(validation_videos[:, :, :, :, 0]) - np.min(validation_videos[:, :, :, :, 0]))\n",
    "#validation_videos[:, :, :, :, 1] = (validation_videos[:, :, :, :, 1] - np.min(validation_videos[:, :, :, :, 1])) / (np.max(validation_videos[:, :, :, :, 1]) - np.min(validation_videos[:, :, :, :, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Definition\n",
    "\n",
    "Next, a CNN model architecture is defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SlowFusion(Model):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(SlowFusion, self).__init__()\n",
    "        \n",
    "        # First level\n",
    "        self._first_layers = [\n",
    "            self._generate_conv_layer(32, (2, 2, 2), (7, 61, 121, 2)),\n",
    "            self._generate_conv_layer(32, (2, 2, 2), (7, 61, 121, 2)),\n",
    "            self._generate_conv_layer(32, (2, 2, 2), (7, 61, 121, 2)),\n",
    "            self._generate_conv_layer(32, (2, 2, 2), (7, 61, 121, 2))\n",
    "        ]\n",
    "        \n",
    "        # Second level\n",
    "        self._second_layers = [\n",
    "            self._generate_conv_layer(16, (2, 2, 2), (6, 30, 60, 32)),\n",
    "            self._generate_conv_layer(16, (2, 2, 2), (6, 30, 60, 32))\n",
    "        ]\n",
    "        \n",
    "        # Third level\n",
    "        self._third_layers = [\n",
    "            self._generate_conv_layer(8, (2, 2, 2), (4, 14, 29, 16))\n",
    "        ]\n",
    "        \n",
    "        # Final Dense layer\n",
    "        self._final_layer = self._generate_dense_layer(13)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        \n",
    "        outputs = self._layer_one(inputs)\n",
    "        #print(outputs[0].shape)\n",
    "        outputs = self._layer_two(outputs)\n",
    "        #print(outputs[0].shape)\n",
    "        outputs = self._layer_three(outputs)\n",
    "        #print(outputs.shape)\n",
    "        outputs = self._final_layer(outputs)\n",
    "        #print(outputs.shape)\n",
    "        return outputs\n",
    "        \n",
    "    def _layer_one(self, inputs):\n",
    "        outputs = []\n",
    "        for n in range(0, 4):\n",
    "            outputs.append(\n",
    "                self._first_layers[n](\n",
    "                    inputs[:, (n*7):((n+1)*7), :, :, :]\n",
    "                )\n",
    "            )\n",
    "        trans_outputs = [\n",
    "            tf.concat((outputs[0], outputs[1]), axis=1),\n",
    "            tf.concat((outputs[2], outputs[3]), axis=1)\n",
    "        ]\n",
    "        return trans_outputs\n",
    "    \n",
    "    def _layer_two(self, inputs):\n",
    "        outputs = []\n",
    "        for n in range(0, 2):\n",
    "            outputs.append(self._second_layers[n](inputs[n]))\n",
    "        trans_outputs = [\n",
    "            tf.concat((outputs[0], outputs[1]), axis=1)\n",
    "        ]\n",
    "        return trans_outputs\n",
    "    \n",
    "    def _layer_three(self, inputs):\n",
    "        return self._third_layers[0](inputs)\n",
    "    \n",
    "    def _generate_conv_layer(self, filters, poolsize, input_shape):\n",
    "        layer = Sequential()\n",
    "        layer.add(Conv3D(\n",
    "            filters, poolsize, input_shape=input_shape,\n",
    "            kernel_initializer=initializers.RandomNormal(mean=0.0, stddev=0.05, seed=None)))\n",
    "        layer.add(Activation('relu'))\n",
    "        #layer.add(LayerNormalization())\n",
    "        layer.add(MaxPooling3D(pool_size=poolsize))\n",
    "        return layer\n",
    "    \n",
    "    def _generate_dense_layer(self, output_size):\n",
    "        layer = Sequential()\n",
    "        layer.add(Flatten())\n",
    "        layer.add(Dense(output_size, kernel_initializer=initializers.RandomNormal(mean=0.0, stddev=0.05, seed=None)))\n",
    "        layer.add(Activation('relu'))\n",
    "        return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_generator(input_shape=(28, 61, 121, 2), learning_rate=0.1):\n",
    "    \"\"\" This method generates a model definition. \"\"\"\n",
    "    model = SlowFusion()\n",
    "    \n",
    "    # Setup training mechanism\n",
    "    model.compile(\n",
    "        loss=\"mean_squared_error\",\n",
    "        optimizer=Adam(learning_rate=learning_rate))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Training\n",
    "\n",
    "Finally, training the model using the single framed data and opening a tensorboard instance with details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(run_name, tensorboard, model, xdata, ydata, models_folder=\"D:/PHD_DATA/Video_18-01-2021/models/\"):\n",
    "    \"\"\" Trains the given model with the given dataset. \"\"\"\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)\n",
    "    history = model.fit(\n",
    "        xdata,\n",
    "        ydata,\n",
    "        batch_size=2,\n",
    "        validation_split=0.3,\n",
    "        callbacks=[tensorboard, es],\n",
    "        epochs=100\n",
    "    )\n",
    "    save_model(model, models_folder + run_name + \".mdl\")\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flexible parameters\n",
    "learning_rate = 0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "96/96 [==============================] - 61s 626ms/step - loss: 776794.0216 - val_loss: 10455.3086\n",
      "Epoch 2/100\n",
      "96/96 [==============================] - 56s 588ms/step - loss: 8375.2390 - val_loss: 9090.4385\n",
      "Epoch 3/100\n",
      "96/96 [==============================] - 55s 574ms/step - loss: 8664.1187 - val_loss: 8928.0742\n",
      "Epoch 4/100\n",
      "96/96 [==============================] - 49s 516ms/step - loss: 8596.5823 - val_loss: 8933.1699\n",
      "Epoch 5/100\n",
      "96/96 [==============================] - 53s 552ms/step - loss: 8426.2554 - val_loss: 8891.4443\n",
      "Epoch 6/100\n",
      "96/96 [==============================] - 55s 570ms/step - loss: 8893.7130 - val_loss: 8914.9248\n",
      "Epoch 7/100\n",
      "96/96 [==============================] - 52s 540ms/step - loss: 9211.3084 - val_loss: 8946.7666\n",
      "Epoch 8/100\n",
      "96/96 [==============================] - 51s 532ms/step - loss: 8913.9747 - val_loss: 9018.6133\n",
      "Epoch 00008: early stopping\n",
      "INFO:tensorflow:Assets written to: D:/PHD_DATA/Video_18-01-2021/models/SF_TEST-1612437294.mdl\\assets\n",
      "Final loss: 9018.61328125\n",
      "Epoch 1/100\n",
      "96/96 [==============================] - 55s 561ms/step - loss: 1056019.1804 - val_loss: 48695.4258\n",
      "Epoch 2/100\n",
      "96/96 [==============================] - 51s 535ms/step - loss: 23643.7074 - val_loss: 6816.5635\n",
      "Epoch 3/100\n",
      "96/96 [==============================] - 52s 544ms/step - loss: 6428.2374 - val_loss: 6762.7100\n",
      "Epoch 4/100\n",
      "96/96 [==============================] - 52s 546ms/step - loss: 5943.4343 - val_loss: 6780.0415\n",
      "Epoch 5/100\n",
      "96/96 [==============================] - 50s 524ms/step - loss: 5959.6255 - val_loss: 6763.5225\n",
      "Epoch 6/100\n",
      "96/96 [==============================] - 52s 545ms/step - loss: 6075.4365 - val_loss: 6773.7280\n",
      "Epoch 00006: early stopping\n",
      "INFO:tensorflow:Assets written to: D:/PHD_DATA/Video_18-01-2021/models/SF_TEST-1612437737.mdl\\assets\n",
      "Final loss: 6773.72802734375\n",
      "Epoch 1/100\n",
      "96/96 [==============================] - 56s 576ms/step - loss: 2368909.2178 - val_loss: 745716.8125\n",
      "Epoch 2/100\n",
      "96/96 [==============================] - 51s 532ms/step - loss: 559397.5757 - val_loss: 227861.6406\n",
      "Epoch 3/100\n",
      "96/96 [==============================] - 53s 552ms/step - loss: 175321.0875 - val_loss: 73214.5625\n",
      "Epoch 4/100\n",
      "96/96 [==============================] - 52s 541ms/step - loss: 55662.2424 - val_loss: 23695.9961\n",
      "Epoch 5/100\n",
      "96/96 [==============================] - 56s 583ms/step - loss: 18785.3163 - val_loss: 10566.2090\n",
      "Epoch 6/100\n",
      "96/96 [==============================] - 52s 544ms/step - loss: 9099.9203 - val_loss: 7720.5264\n",
      "Epoch 7/100\n",
      "96/96 [==============================] - 50s 526ms/step - loss: 7020.2350 - val_loss: 6206.0225\n",
      "Epoch 8/100\n",
      "96/96 [==============================] - 54s 561ms/step - loss: 5589.3432 - val_loss: 5515.8862\n",
      "Epoch 9/100\n",
      "96/96 [==============================] - 51s 536ms/step - loss: 4850.0054 - val_loss: 5259.7031\n",
      "Epoch 10/100\n",
      "96/96 [==============================] - 52s 539ms/step - loss: 4071.8485 - val_loss: 3554.6223\n",
      "Epoch 11/100\n",
      "96/96 [==============================] - 55s 577ms/step - loss: 3125.8711 - val_loss: 3516.5481\n",
      "Epoch 12/100\n",
      "96/96 [==============================] - 53s 547ms/step - loss: 3382.6202 - val_loss: 3485.7002\n",
      "Epoch 13/100\n",
      "96/96 [==============================] - 57s 589ms/step - loss: 3187.7978 - val_loss: 3478.6228\n",
      "Epoch 14/100\n",
      "96/96 [==============================] - 53s 552ms/step - loss: 3408.7806 - val_loss: 3473.8235\n",
      "Epoch 15/100\n",
      "96/96 [==============================] - 53s 558ms/step - loss: 3062.7625 - val_loss: 3485.8167\n",
      "Epoch 16/100\n",
      "96/96 [==============================] - 57s 596ms/step - loss: 3124.0570 - val_loss: 3462.8162\n",
      "Epoch 17/100\n",
      "96/96 [==============================] - 74s 776ms/step - loss: 3358.0516 - val_loss: 3452.8542\n",
      "Epoch 18/100\n",
      "96/96 [==============================] - 68s 704ms/step - loss: 3324.8919 - val_loss: 3462.5911\n",
      "Epoch 19/100\n",
      "96/96 [==============================] - 69s 724ms/step - loss: 2861.6266 - val_loss: 3445.7415\n",
      "Epoch 20/100\n",
      "96/96 [==============================] - 72s 757ms/step - loss: 3107.7676 - val_loss: 3444.1201\n",
      "Epoch 21/100\n",
      "96/96 [==============================] - 68s 713ms/step - loss: 3188.1204 - val_loss: 3450.1008\n",
      "Epoch 22/100\n",
      "96/96 [==============================] - 64s 667ms/step - loss: 3187.1779 - val_loss: 3447.3320\n",
      "Epoch 23/100\n",
      "96/96 [==============================] - 69s 720ms/step - loss: 2968.4898 - val_loss: 3446.9575\n",
      "Epoch 00023: early stopping\n",
      "INFO:tensorflow:Assets written to: D:/PHD_DATA/Video_18-01-2021/models/SF_TEST-1612438062.mdl\\assets\n",
      "Final loss: 3446.95751953125\n",
      "Epoch 1/100\n",
      "96/96 [==============================] - 80s 804ms/step - loss: 10598178.8918 - val_loss: 2316670.7500\n",
      "Epoch 2/100\n",
      "96/96 [==============================] - 75s 781ms/step - loss: 1611663.0760 - val_loss: 444943.5312\n",
      "Epoch 3/100\n",
      "96/96 [==============================] - 77s 800ms/step - loss: 328195.4555 - val_loss: 112672.9375\n",
      "Epoch 4/100\n",
      "96/96 [==============================] - 74s 772ms/step - loss: 85186.4441 - val_loss: 34205.5742\n",
      "Epoch 5/100\n",
      "96/96 [==============================] - 71s 745ms/step - loss: 26537.8001 - val_loss: 14041.0967\n",
      "Epoch 6/100\n",
      "96/96 [==============================] - 68s 713ms/step - loss: 12183.9747 - val_loss: 9375.0576\n",
      "Epoch 7/100\n",
      "96/96 [==============================] - 64s 668ms/step - loss: 8091.5827 - val_loss: 8434.9785\n",
      "Epoch 8/100\n",
      "96/96 [==============================] - 68s 713ms/step - loss: 7523.7679 - val_loss: 8283.5703\n",
      "Epoch 9/100\n",
      "96/96 [==============================] - 67s 698ms/step - loss: 7950.3134 - val_loss: 8296.9512\n",
      "Epoch 10/100\n",
      "96/96 [==============================] - 69s 719ms/step - loss: 7110.3297 - val_loss: 8268.7715\n",
      "Epoch 11/100\n",
      "96/96 [==============================] - 75s 776ms/step - loss: 7669.4658 - val_loss: 8259.1318\n",
      "Epoch 12/100\n",
      "96/96 [==============================] - 66s 688ms/step - loss: 7451.1571 - val_loss: 8335.8262\n",
      "Epoch 13/100\n",
      "96/96 [==============================] - 78s 810ms/step - loss: 7548.6273 - val_loss: 8272.4883\n",
      "Epoch 14/100\n",
      "96/96 [==============================] - 73s 759ms/step - loss: 7091.4458 - val_loss: 8282.9434\n",
      "Epoch 00014: early stopping\n",
      "INFO:tensorflow:Assets written to: D:/PHD_DATA/Video_18-01-2021/models/SF_TEST-1612439414.mdl\\assets\n",
      "Final loss: 8282.943359375\n",
      "Epoch 1/100\n",
      "96/96 [==============================] - 86s 855ms/step - loss: 3693825.7938 - val_loss: 965687.8125\n",
      "Epoch 2/100\n",
      "96/96 [==============================] - 66s 684ms/step - loss: 677759.3057 - val_loss: 188552.5312\n",
      "Epoch 3/100\n",
      "96/96 [==============================] - 64s 665ms/step - loss: 138677.8476 - val_loss: 48145.3359\n",
      "Epoch 4/100\n",
      "96/96 [==============================] - 64s 663ms/step - loss: 36795.5330 - val_loss: 14914.0674\n",
      "Epoch 5/100\n",
      "96/96 [==============================] - 64s 665ms/step - loss: 11547.2377 - val_loss: 8655.0439\n",
      "Epoch 6/100\n",
      "96/96 [==============================] - 68s 711ms/step - loss: 7878.0697 - val_loss: 8174.4155\n",
      "Epoch 7/100\n",
      "78/96 [=======================>......] - ETA: 12s - loss: 7236.1942"
     ]
    }
   ],
   "source": [
    "# Run each model multiple times\n",
    "for i in range(0, 5):\n",
    "    run_name = \"SF_TEST-{}\".format(int(time.time()))#\"SF_32_16_8__13__222_t-{}\".format(int(time.time()))\n",
    "    tb = TensorBoard(log_dir=\".\\\\logs\\\\{}\".format(run_name))\n",
    "    model = model_generator(learning_rate=learning_rate, input_shape=training_videos.shape[1:])\n",
    "    history = train_model(run_name, tb, model, training_videos, training_rainfall)\n",
    "    print(\"Final loss: {}\".format(history.history[\"val_loss\"][-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
