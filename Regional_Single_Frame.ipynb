{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Frame Model\n",
    "\n",
    "This script trains a single frame model based on an average image taken across the the forcasted month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, save_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, Conv3D, MaxPooling2D, MaxPooling3D, LayerNormalization\n",
    "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import initializers\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Define Region\n",
    "\n",
    "First, define a region to train a model for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGIONS = [\"ES\",\"NS\",\"WS\",\"EM\",\"EE\",\"LD\",\"NEE\",\"NWE\",\"SEE\",\"SWE\",\"WAL\",\"WM\",\"YH\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data\n",
    "\n",
    "To begin, data for a single frame mnodel must be loaded along with the expected regional rainfall values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.0 New Training Data\n",
    "\n",
    "This section loads the new training data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_datafile = \"D:/PHD_DATA/Video_18-01-2021/prepared-data/single_all.npy\"\n",
    "training_rainfallfile = \"D:/PHD_DATA/Video_18-01-2021/prepared-data/expected_all.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_videos = np.load(training_datafile)\n",
    "training_rainfall = np.load(training_rainfallfile)[:, 2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_videos = np.swapaxes(training_videos, 1, 2)\n",
    "training_videos = np.swapaxes(training_videos, 2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_videos[:, :, :, 0] = (training_videos[:, :, :, 0] - np.min(training_videos[:, :, :, 0])) / (np.max(training_videos[:, :, :, 0]) - np.min(training_videos[:, :, :, 0]))\n",
    "training_videos[:, :, :, 1] = (training_videos[:, :, :, 1] - np.min(training_videos[:, :, :, 1])) / (np.max(training_videos[:, :, :, 1]) - np.min(training_videos[:, :, :, 1]))\n",
    "\n",
    "for n in range(0, len(REGIONS)):\n",
    "    training_rainfall[:, n] = (training_rainfall[:, n] - np.min(training_rainfall[:, n])) / (np.max(training_rainfall[:, n]) - np.min(training_rainfall[:, n]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(np.min(training_rainfall, axis=0))\n",
    "print(np.max(training_rainfall, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Definition\n",
    "\n",
    "Next, a CNN model architecture is defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_generator(input_shape=(2, 61, 121), learning_rate=0.1):\n",
    "    \"\"\" This method generates a model definition. \"\"\"\n",
    "    model = Sequential()\n",
    "    \n",
    "    # First layer\n",
    "    model.add(Conv2D(8, (2, 2), input_shape=input_shape))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(LayerNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    # Second layer\n",
    "    model.add(Conv2D(8, (2, 2), input_shape=input_shape))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(LayerNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    # Third layer\n",
    "    model.add(Conv2D(8, (2, 2), input_shape=input_shape))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(LayerNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    # Final Layer\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    # Setup training mechanism\n",
    "    model.compile(\n",
    "        loss=\"mean_squared_error\",\n",
    "        optimizer=Adam(learning_rate=learning_rate))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Training\n",
    "\n",
    "Finally, training the model using the single framed data and opening a tensorboard instance with details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(run_name, tensorboard, model, xdata, ydata, models_folder=\"D:/PHD_DATA/Video_18-01-2021/models/\"):\n",
    "    \"\"\" Trains the given model with the given dataset. \"\"\"\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n",
    "    history = model.fit(\n",
    "        xdata,\n",
    "        ydata,\n",
    "        batch_size=8,\n",
    "        validation_split=0.3,\n",
    "        callbacks=[tensorboard, es],\n",
    "        epochs=100\n",
    "    )\n",
    "    save_model(model, models_folder + run_name + \".mdl\")\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flexible parameters\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    }
   ],
   "source": [
    "final_errors = {}\n",
    "# Run model for each region\n",
    "for ridx, r in enumerate(REGIONS):\n",
    "    final_errors[r] = {}\n",
    "    # Run each model multiple times\n",
    "    for i in range(0, 3):\n",
    "        run_name = \"{}-{}\".format(r, int(time.time()))\n",
    "        tb = TensorBoard(log_dir=\"D:/PHD_DATA/Video_11-02-2021/SingleFrame/logs/{}\".format(run_name))\n",
    "        model = model_generator(learning_rate=learning_rate, input_shape=training_videos.shape[1:])\n",
    "        history = train_model(run_name, tb, model, training_videos, training_rainfall[:, ridx],\n",
    "                              models_folder=\"D:/PHD_DATA/Video_11-02-2021/SingleFrame/models/\")\n",
    "        final_errors[r][run_name] = history.history[\"val_loss\"][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
